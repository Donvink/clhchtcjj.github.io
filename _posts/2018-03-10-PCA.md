---
layout: post
title:  PCA算法
categories: machine-learning
author: CC
tags: machine-learning
---

# PCA

主成分分析(Principe Component Analysis，PCA)，属于最常用的降维方法之一。顾名思义，主成分分析就是找出数据的主要部分来替代表示原始数据。

所谓主要部分，就是用它来表示原始数据时，误差小。

那么如何衡量这个误差呢？

首先，将数据从高维降低到低维，实际上就是找一个超平面，使其能够对所有样本进行恰当的表达。

那么，PCA主要从两个角度来衡量：

- 最近重构性：样本点到这个超平面的距离都足够接近
- 最大可分性：样本点在这个超平面上的投影能尽可能分开



## 1. 算法流程

**输入：**n维样本集$D=({\bf{x}}^{(1)}, {\bf{x}}^{(2)},...,{\bf{x}}^{(m)})$

**输出：**降维后的样本集$D'$，维度为$d'$

（1） 对所有样本进行去中心化：${\bf{x}}^{(i)} = {\bf{x}}^{(i)} - \frac{1}{m}\sum\limits_{j=1}^{m} {\bf{x}}^{(i)}$，即$\sum_ix^{(i)} = 0$

（2）计算样本的协方差矩阵：$XX^T$

（3）对协方差矩阵做特征值分解

（4）取最大的$d'$个特征值所对应的特征向量：

​						${\bf{w}}_1,{\bf{w}}_2,...,{\bf{w}}_{d‘}$

​         将所有的特征向量标准化后，组成特征向量矩阵 ${\bf{W}}$，或

​         者按阈值$t$取：$\frac{\sum_{i=1}^{n'}\lambda_i}{\sum_{i=1}^{n}\lambda_i} \geq t $

（5）对样本集中的每一个样本$x^{(i)}$,转化为新的样本$z^{(i)}=W^T{\bf{x}}^{(i)}$

（6）得到输出样本集$D' =({\bf{z}}^{(1)}, {\bf{z}}^{(2)},...,{\bf{z}}^{(m)})$



## 2. 问题定义与符号说明

首先，假设数据样本$D=({\bf{x}}^{(1)}, {\bf{x}}^{(2)},...,{\bf{x}}^{(m)})$进行过中心化，即$\sum_i{\bf{x}}^{(i)} = \bf{0}$；经过投影变换后的新坐标系为{${\bf{w}}_1,{\bf{w}}_2,...,{\bf{w}}_n$}，其中$w_i$均为标准正交基，即：$\lvert\lvert{\bf{w}}_i\lvert\lvert_2=1$, ${\bf{w}}_i^T{\bf{w}}_j=0$,$(i\neq j)$。

假设，原始数据的维度是$d$，那么$ {\bf{W}}：n\times n$、${\bf{z}_i}：d'\times 1$、${\bf{x}}^{(i)} : n\times 1$、${\bf{\hat{x}}}^{(i)}：d'\times 1 $

明确几个简单概念：

- 正交基：{${\bf{w}}_1,{\bf{w}}_2,...,{\bf{w}}_n$}

- 原始数据：${\bf{x}}^{(i)} = \sum_j^nz_{ij}{\bf{w}}_j$ => ${\bf{x}}^{(i)}= {\bf{W}}^T{\bf{z}}_i$

- 基坐标/投影：$z_{ij}={\bf{w}}_j^T{\bf{x}}^{(i)}$ =>  ${\bf{z}_i} = {\bf{W}}^T{\bf{x}}^{(i)}$

- 方差：

  ​	$\frac{1}{m}\sum_{i=1}^m\lvert\lvert{\bf{W}}^T{\bf{x}}^{(i)}\lvert\lvert^2_2$

  ​					$=\frac{1}{m}\sum_{i=1}^m({\bf{W}}^T{\bf{x}}^{(i)})^T({\bf{W}}^T{\bf{x}}^{(i)})$

  ​					$=\frac{1}{m}\sum_{i=1}^mtr(({\bf{W}}^T{\bf{x}}^{(i)})({\bf{W}}^T{\bf{x}}^{(i)})^T)$

  ​					$=tr({\bf{W}}^TS{\bf{W}})$

  ​	$S=\frac{1}{m}\sum_i{\bf{x}}^{(i)}{\bf{x}}^{(i)T}=\frac{1}{m}{\bf{X}}{\bf{X}}^T​$

- 降维重建后的数据为：${\bf{\hat{x}}}^{(i)} = \sum_j^{d'}z_{ij}{\bf{w}}_j$

- $\sum_i{\bf{x}}_i{\bf{x}}_i^T={\bf{X}}{\bf{X}}^T$





## 3. PCA的推导与解释

## 3.1 最近重构性

![](https://raw.githubusercontent.com/clhchtcjj/Pit-for-Typora/master/PCA-1.jpg)

## 3.2 最大可分性

![](https://raw.githubusercontent.com/clhchtcjj/Pit-for-Typora/master/PCA-2.jpg)

## 3.3 降维 

![](https://raw.githubusercontent.com/clhchtcjj/Pit-for-Typora/master/PCA-3.jpg)



## 4. KPCA

PCA算法中存在一个假设：存在一个线性的超平面，可以对数据进行投影。

那当数据不是线性可分的时候，便不能直接进行PCA降维。

核主成分分析（kernel PCA）可以用于对非线性数据进行降维。它借鉴了和SVM一样的核函数的思想：先把数据从$n$维映射到线性可分的更高维空间$N$，然后从$N$维降低到低维$d'$。其中，$d'<n<N$。

则对于n维空间的特征分解：

​				$$\sum\limits_{i=1}^{m}x^{(i)}x^{(i)T}W=\lambda W$$

映射为：

​				$$\sum\limits_{i=1}^{m}\phi({\bf{x}}^{(i)})\phi({\bf{x}}^{(i)})^TW=\lambda W$$

通过在高维空间进行协方差矩阵的特征值分解，然后用和PCA一样的方法进行降维。一般来说，映射$\phi$不用显式的计算，而是在需要计算的时候通过核函数完成。由于KPCA需要核函数的运算，因此它的计算量要比PCA大很多。



## 5. PCA算法实现

```python
# 手动实现PCA
def my_pca(X, k):
    m, n = X.shape
    # 对每个特征求均值
    mean = np.array([np.mean(X[:,i]) for i in range(n)])
    # n * m
    X_norm = np.transpose(X - mean)
    print(X_norm)
    # 获得协方差矩阵
    XXT = np.dot(X_norm, np.transpose(X_norm))
    # 求解特征值及其对应的特征向量
    eig_val,eig_vec = np.linalg.eig(XXT)
    eig_pairs = [(np.abs(eig_val[i]),eig_vec[:,i]) for i in range(n)]
    print(eig_pairs)
    # 从大到小排序
    eig_pairs.sort(reverse=True)
    # 选择主特征
    # n * k
    feature = np.transpose(np.array([ele[1] for ele in eig_pairs[:k]]))
    # 降维后的数据
    data = np.dot(np.transpose(feature),X_norm)
    return data.T
```

[sklearn版本](https://github.com/clhchtcjj/Machine-Learning/blob/master/dimensionality%20reduction/PCA.py)

## 6. PCA算法总结

- 算法类型：非监督
- 应用场景：应用最为广泛的是人脸识别。总的来说，PCA是一种粗糙的降维方法，对于维度不是很高的小数据集来说，效果可能不如因子分解法，但是当数据维度较高时，PCA效果不错。
- 不适用的场景：PCA是解除数据间的线性相关，但是当数据中存在高阶的线性相关性时，PCA无能为力，这时候可以考虑KPCA
- 优点：
  - 仅需要计算数据协方差矩阵的特征值，计算简单，易于实现
  - 可以消除原始数据成分之间相互影响的因素，便于数据处理与分析
- 缺点：
  - 不具有解释性
  - 丢弃的方差小的非主成分（参考3.3）可能含有对样本差异的重要信息，丢弃后可能影响后面的数据分析




## 7. 主成分回归

主成分回归分析（Principal Component Regression ，PCR），即以主成分为自变量进行回归分析。

是分析多元共线性问题的一种方法。

用PCA对回归模型中的多重共线性问题进行消除后，将主成分变量作为自变量进行回归分析，然后根据得分系数将原变量带回得到新的模型。

**缺点：利用主成分得到的回归关系不像用原自变量建立的回归关系那么容易解释。**



## 8. 两组数据主成分提取

在主成分分析介绍了如何提取单个数据源（一个数据矩阵）的主成分，使得提取的主成分能够尽可能多的携带原数据的信息，能对原数据表的变异情况具有最强的解释能力。

当存在两组数据时，通常是分别提取两组数据中的主要成分，以分析主要成分之间的相关性来**分析两组变量之间是否存在相关性**，常用的算法有：典型相关性分析和偏最小二乘回归（Partial Least Squares Regression ，PLSR）。

### 8.1 典型相关性分析

**典型相关分析**（canonical correlation analysis）是对互[协方差矩阵](https://baike.baidu.com/item/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5)的一种理解，是利用综合变量对两组指标（特征）之间整体相关性的多元统计分析方法。

基本原理是：在两组变量中提取有代表性的两个综合变量${\bf{u}}$和${\bf{v}}$（分别为两个变量组中各变量的线性组合）。利用这两个综合变量之间的相关关系来反应两组指标之间的整体相关性。

具体地，首先在每组变量中找出第一对线性组合，使其具有最大的相关性，然后找出第二对线性组合，使其分别与本组内的第一对线性组合不相关，第二对线性组合本身具有次大的相关性。

不断进行下去，直至两组变量相关性被提取完毕，可以得到r组变量，r小于等于两组变量中最小指标的个数。

**每个成分都是独立分析的**

### 8.2 偏最小二乘回归分析

设有$q$个因变量：

​								{${\bf{y}}_1,{\bf{y}}_2,...,{\bf{y}}_q$}

和$p$个自变量：

​								{${\bf{x}}_1,{\bf{x}}_2,...,{\bf{x}}_p$}

为了研究这两组变量的统计关系，我们观测了$n$个样本点，由此构成了两个数据矩阵：

​							$X=[x_1,x2,...,x_p]_{n\times p}$

​							$Y=[y_1,y_2,...,y_q]_{n\times q}$

偏最小二乘回归分别在$X$和$Y$中提取成分${\bf{u}}_1$和${\bf{v}}_1$。

其中，${\bf{u}}_1$和${\bf{v}}_1$分别是${\bf{x}}_1,{\bf{x}}_2,...,{\bf{x}}_p$和${\bf{y}}_1,{\bf{y}}_2,...,{\bf{y}}_q$的线性组合

在提取这两个成分时，为了回归分析的需要，有下面两个要求：

（1）${\bf{u}}_1$和${\bf{v}}_1$尽可能大地携带它们各自数据中的变异信息；

（2）${\bf{u}}_1$和${\bf{v}}_1$的相关度能够达到最大。

在第一个成分${\bf{u}}_1$和${\bf{v}}_1$被提取后，偏最小二乘回归分析分别实施$X$对${\bf{u}}_1$的回归以及$Y$对${\bf{u}}_1$的回归，如果回归方程已经达到满意的精度，则算法终止；否则，将利用$X$被${\bf{u}}_1$解释后的残余信息以及$Y$被${\bf{u}}_1$解释后的残余信息进行第二轮的成分提取，如此往复，直到能达到一个较满意的精度为止。若最终$X$共提取了$m$个成分$${\bf{u}}_1,{\bf{u}}_2,...,{\bf{u}}_m$$，偏最小二乘将通过对${\bf{y}}_k$的对$${\bf{u}}_1,{\bf{u}}_2,...,{\bf{u}}_m$$的回归，然后再表示成${\bf{y}}_k$的对$${\bf{x}}_1,{\bf{x}}_2,...,{\bf{x}}_m$$的回归方程。

**所有成分综合起来分析**

**显然，CCA只能对$X​$与$Y​$进行相关性分析，而PLSR可以找出$X​$到$Y​$的映射关系**



## 参考文献

【1】 周志华 《机器学习》

【2】 刘建平 [主成分分析（PCA）原理总结](https://github.com/clhchtcjj/Pit-for-Typora/blob/master/PCA-1.jpg)

【3】JerryLead [偏最小二乘法回归](http://www.cnblogs.com/jerrylead/archive/2011/08/21/2148625.html) 

